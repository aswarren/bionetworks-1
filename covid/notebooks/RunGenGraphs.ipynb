{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from screed import ScreedDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Virus name', 'Type', 'Accession ID', 'Collection date', 'Location',\n",
      "       'Additional location information', 'Sequence length', 'Host',\n",
      "       'Patient age', 'Gender', 'Clade', 'Pango lineage', 'Pangolin version',\n",
      "       'Variant', 'AA Substitutions', 'Submission date', 'Is reference?',\n",
      "       'Is complete?', 'Is high coverage?', 'Is low coverage?', 'N-Content',\n",
      "       'GC-Content'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "metadata = pd.read_csv(\"../data/metadata_2021_04_08.tsv\", sep=\"\\t\", parse_dates = [\"Collection date\", \"Submission date\"])\n",
    "print(metadata.columns)\n",
    "metadata = metadata[[\"Accession ID\", \\\n",
    "                   \"Collection date\", \\\n",
    "                   \"Submission date\", \\\n",
    "                   \"Location\", \\\n",
    "                   \"Additional location information\", \\\n",
    "                   \"Sequence length\", \\\n",
    "                   \"Host\", \\\n",
    "                   \"AA Substitutions\", \\\n",
    "                   \"Is reference?\", \\\n",
    "                   \"Pango lineage\"]]\n",
    "def get_nth_slash(row, n):\n",
    "    try:\n",
    "        return row.split(\"/\")[n].strip()\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "for i in range(4):\n",
    "    metadata[f\"Location_{i}\"] = metadata[\"Location\"].apply(lambda row: get_nth_slash(row, i))\n",
    "metadata = metadata.rename(columns={\"Location_1\":\"country\", \"Location_2\":\"state\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B.1.2        63442\n",
       "B.1          30444\n",
       "B.1.429      16400\n",
       "B.1.1.7      15248\n",
       "B.1.596       8439\n",
       "B.1.243       8077\n",
       "B.1.427       6871\n",
       "B.1.234       4681\n",
       "B.1.1.519     4359\n",
       "B.1.526       4187\n",
       "B.1.1         4170\n",
       "A.1           2518\n",
       "B.1.595       2421\n",
       "B.1.240       2417\n",
       "B.1.311       2170\n",
       "B.1.369       2003\n",
       "B.1.526.1     1851\n",
       "B.1.1.434     1773\n",
       "B.1.1.222     1648\n",
       "B.1.517       1524\n",
       "B.1.526.2     1375\n",
       "B.1.564       1272\n",
       "B.1.400       1224\n",
       "B.1.577       1177\n",
       "B.1.139       1161\n",
       "B.1.1.291     1130\n",
       "B.1.582        986\n",
       "B.1.232        954\n",
       "A              954\n",
       "B.1.575        914\n",
       "B.1.396        870\n",
       "B.1.426        867\n",
       "B.1.561        861\n",
       "B.1.588        856\n",
       "B.1.206        839\n",
       "B.1.375        831\n",
       "B.1.409        827\n",
       "B.1.349        822\n",
       "B.1.509        810\n",
       "B.1.609        796\n",
       "B              776\n",
       "B.1.371        752\n",
       "P.2            743\n",
       "B.1.565        662\n",
       "B.1.320        650\n",
       "B.1.1.316      649\n",
       "R.1            634\n",
       "B.1.241        604\n",
       "B.1.590        582\n",
       "B.1.361        577\n",
       "Name: Pango lineage, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[metadata.country == \"USA\"][\"Pango lineage\"].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_gengraph_input_output(date, end_date, seq_names):\n",
    "    date = date.strftime(\"%Y-%m-%d\")\n",
    "    end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    output_path = Path(\"/sfs/lustre/bahamut/scratch/jho5ze/bionets/covid/data/Houston_output\")\n",
    "    run_path = output_path / f\"{date}_{end_date}\"\n",
    "    \n",
    "    if not run_path.exists():\n",
    "        run_path.mkdir()\n",
    "        \n",
    "    seq_path = Path(\"/sfs/lustre/bahamut/scratch/jho5ze/bionets/covid/data/Houston\")\n",
    "    seq_file_dest = run_path / f\"{date}_{end_date}_seq_input.txt\"\n",
    "    with open(seq_file_dest, \"w\") as dest:\n",
    "        dest.write(\"seq_name\\taln_name\\tseq_path\\tannotation_path\\n\")\n",
    "        for ix, seq in enumerate(seq_names):\n",
    "            dest.write(f\"{seq}\\tseq_{ix}\\t{seq_path / seq}.fasta\\n\")\n",
    "            \n",
    "    return run_path, seq_file_dest, f\"{date}_{end_date}\"\n",
    "\n",
    "def run_pipeline(date, end_date, seq_names, exp_name, location=\"Houston\"):\n",
    "    location = location.replace(\" \", \"_\")\n",
    "    base_path = Path(\"/scratch/jho5ze/bionets/covid/\")\n",
    "    uncert_file = \"uncert.csv\"\n",
    "    \n",
    "    date = date.strftime(\"%Y-%m-%d\")\n",
    "    end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    output_path = Path(\"/sfs/lustre/bahamut/scratch/jho5ze/bionets/covid/data/output\")\n",
    "    run_path = output_path / location / exp_name / f\"{date}_{end_date}\"\n",
    "    uncert_file = run_path / \"uncert.csv\"\n",
    "    \n",
    "    \n",
    "    if not run_path.exists():\n",
    "        run_path.mkdir(parents=True)\n",
    "        \n",
    "    seq_path = Path(\"/sfs/lustre/bahamut/scratch/jho5ze/bionets/covid/data/Houston\")\n",
    "    \n",
    "    command = f\"sbatch {base_path / 'scripts/run_pipeline.sbatch'} {run_path} {uncert_file}\".split()\n",
    "#     command += [f\"{seq_path / seq}.fasta\" for seq in seq_names]\n",
    "    command += [f\"{seq}\" for seq in seq_names]\n",
    "    return command\n",
    "\n",
    "\n",
    "def run_jaccard_pipeline(date, end_date, seq_names, exp_name, location=\"Houston\"):\n",
    "    location = location.replace(\" \", \"_\")\n",
    "    base_path = Path(\"/scratch/jho5ze/bionets/covid/\")\n",
    "    uncert_file = \"uncert.csv\"\n",
    "    \n",
    "    date = date.strftime(\"%Y-%m-%d\")\n",
    "    end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    output_path = Path(\"/sfs/lustre/bahamut/scratch/jho5ze/bionets/covid/data/output\")\n",
    "    run_path = output_path / location / exp_name / f\"{date}_{end_date}\"\n",
    "    uncert_file = run_path / \"jaccard_uncert.csv\"\n",
    "    \n",
    "    \n",
    "    if not run_path.exists():\n",
    "        run_path.mkdir(parents=True)\n",
    "        \n",
    "    seq_path = Path(\"/sfs/lustre/bahamut/scratch/jho5ze/bionets/covid/data/Houston\")\n",
    "    \n",
    "    command = f\"sbatch {base_path / 'scripts/run_jaccard_pipeline.sbatch'} {location} {date} {end_date} {uncert_file}\".split()\n",
    "#     command += [f\"{seq_path / seq}.fasta\" for seq in seq_names]\n",
    "    command += [f\"{seq}\" for seq in seq_names]\n",
    "    return command\n",
    "#     subprocess.run(command)\n",
    "#     with open(seq_file_dest, \"w\") as dest:\n",
    "#         dest.write(\"seq_name\\taln_name\\tseq_path\\tannotation_path\\n\")\n",
    "#         for ix, seq in enumerate(seq_names):\n",
    "#             dest.write(f\"{seq}\\tseq_{ix}\\t{seq_path / seq}.fasta\\n\")\n",
    "            \n",
    "#     return run_path, seq_file_dest, f\"{date}_{end_date}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout = \"\"\"Houston 2020-06-03 2020-06-09\n",
    "Houston 2020-06-10 2020-06-16\n",
    "Houston 2020-06-17 2020-06-23\n",
    "Houston 2020-06-24 2020-06-30\n",
    "Houston 2020-07-01 2020-07-07\n",
    "Houston 2020-07-08 2020-07-14\n",
    "Houston 2020-08-12 2020-08-18\n",
    "Houston 2020-08-19 2020-08-25\n",
    "Houston 2020-08-26 2020-09-01\n",
    "Houston 2020-09-23 2020-09-29\n",
    "Houston 2020-09-30 2020-10-06\n",
    "Houston 2020-10-07 2020-10-13\n",
    "Houston 2020-10-14 2020-10-20\n",
    "Houston 2020-10-21 2020-10-27\n",
    "Houston 2020-10-28 2020-11-03\n",
    "Houston 2020-11-04 2020-11-10\n",
    "Houston 2020-11-11 2020-11-17\n",
    "Houston 2020-11-18 2020-11-24\n",
    "Houston 2020-11-25 2020-12-01\n",
    "Houston 2020-12-23 2020-12-29\n",
    "Houston 2020-12-30 2021-01-05\n",
    "Houston 2021-01-06 2021-01-12\n",
    "Houston 2021-01-13 2021-01-19\n",
    "Houston 2021-01-20 2021-01-26\n",
    "Houston 2021-01-27 2021-02-02\n",
    "Houston 2021-02-03 2021-02-09\n",
    "Houston 2021-02-10 2021-02-16\n",
    "Houston 2021-02-17 2021-02-23\n",
    "Houston 2021-02-24 2021-03-02\n",
    "Houston 2021-03-03 2021-03-09\n",
    "New_York_City 2021-02-01 2021-02-07\n",
    "New_York_City 2021-02-08 2021-02-14\n",
    "New_York_City 2021-02-15 2021-02-21\n",
    "New_York_City 2021-02-22 2021-02-28\n",
    "New_York_City 2021-03-01 2021-03-07\n",
    "New_York_City 2021-03-08 2021-03-14\n",
    "New_York_City 2021-03-15 2021-03-21\n",
    "New_York_City 2021-03-22 2021-03-28\n",
    "New_York_City 2021-03-29 2021-04-04\n",
    "San_Diego 2021-01-16 2021-01-22\n",
    "Santa_Clara_County 2020-12-02 2020-12-08\n",
    "Yakima_County 2020-05-26 2020-06-01\n",
    "Yakima_County 2020-06-02 2020-06-08\"\"\".split(\"\\n\")\n",
    "# timeout = \"New_York_City 2021-02-22_2021-02-28\"\n",
    "# timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/scratch/jho5ze/bionets/covid/data\")\n",
    "\n",
    "seqs = pd.read_csv(data_path / \"houston_metadata.csv\", header=None, parse_dates=[1])\n",
    "seqs.columns = [\"seq\", \"date\"]\n",
    "\n",
    "date_window = 7\n",
    "date_step_size = 7\n",
    "\n",
    "for date in pd.date_range(seqs.date.min(), seqs.date.max(), freq=f\"{date_step_size}D\"):\n",
    "    end_date = date + np.timedelta64(date_window - 1, 'D')\n",
    "    sub_seqs = seqs[(seqs.date >= date) & (seqs.date <= end_date)].seq\n",
    "#     output_directory, seq_file, out_file = prep_gengraph_input_output(date, end_date, sub_seqs)\n",
    "    experiment_name = f\"window_{date_window}_step_{date_step_size}\"\n",
    "    stdate = date.strftime(\"%Y-%m-%d\")\n",
    "    stend_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "#     if f\"{stdate}_{stend_date}\" in timeout:\n",
    "#     command = run_pipeline(date, end_date, sub_seqs, experiment_name)\n",
    "#     subprocess.run(command)\n",
    "#     print(f\"{stdate}_{stend_date}\")\n",
    "#     print(\"\\t\", len(sub_seqs))\n",
    "#     break\n",
    "#     print(\" \".join(command))\n",
    "#     break\n",
    "        \n",
    "#         command = f\"sbatch {base_path / 'scripts/run_gengraph.sbatch'} {output_directory} {seq_file} {out_file}\"\n",
    "#         subprocess.run(command.split())\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246062,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[metadata.country == \"USA\"].Location_3.shape #value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139992"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[metadata.country == \"USA\"].Location_3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accession ID</th>\n",
       "      <th>Collection date</th>\n",
       "      <th>Submission date</th>\n",
       "      <th>Location</th>\n",
       "      <th>Additional location information</th>\n",
       "      <th>Sequence length</th>\n",
       "      <th>Host</th>\n",
       "      <th>AA Substitutions</th>\n",
       "      <th>Is reference?</th>\n",
       "      <th>Location_0</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>Location_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>830371</th>\n",
       "      <td>EPI_ISL_1303700</td>\n",
       "      <td>2021-02-24</td>\n",
       "      <td>2021-03-21</td>\n",
       "      <td>North America / USA / Texas / Houston</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29859</td>\n",
       "      <td>Human</td>\n",
       "      <td>(NSP6_G107del,N_S194L,NSP6_S106del,Spike_E484K...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>North America</td>\n",
       "      <td>USA</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Accession ID Collection date Submission date  \\\n",
       "830371  EPI_ISL_1303700      2021-02-24      2021-03-21   \n",
       "\n",
       "                                     Location Additional location information  \\\n",
       "830371  North America / USA / Texas / Houston                             NaN   \n",
       "\n",
       "        Sequence length   Host  \\\n",
       "830371            29859  Human   \n",
       "\n",
       "                                         AA Substitutions Is reference?  \\\n",
       "830371  (NSP6_G107del,N_S194L,NSP6_S106del,Spike_E484K...           NaN   \n",
       "\n",
       "           Location_0 country  state Location_3  \n",
       "830371  North America     USA  Texas    Houston  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[metadata[\"Accession ID\"] == \"EPI_ISL_1303700\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "msadb = ScreedDB(\"../data/msa_0408/usa_msa_0408.fasta\")\n",
    "# msadb_keys = msadb.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "top_k_sequenced_us = metadata[metadata.country == \"USA\"].Location_3.value_counts()[:top_k].index\n",
    "top_state_county = []\n",
    "for i in top_k_sequenced_us:\n",
    "    top_state = metadata[(metadata.country == \"USA\") & (metadata.Location_3 == i)].state.value_counts().index[0]\n",
    "#     print(i, \":\", top_state)\n",
    "#     print()\n",
    "    top_state_county.append((top_state, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Texas', 'Houston'),\n",
       " ('New York', 'New York City'),\n",
       " ('California', 'San Diego'),\n",
       " ('California', 'Santa Clara County'),\n",
       " ('Wisconsin', 'Dane County'),\n",
       " ('California', 'Alameda County'),\n",
       " ('Washington', 'Yakima County'),\n",
       " ('Washington', 'King County'),\n",
       " ('California', 'Orange County'),\n",
       " ('California', 'Los Angeles County')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_state_county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_date_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location = \"Houston\"\n",
    "\"\"\"\n",
    "Need to redo Orange county (had a few NY sequences and a few others in there too)\n",
    "\"\"\"\n",
    "for state, location in top_state_county:\n",
    "#     if location == \"Houston\":\n",
    "#         continue\n",
    "        \n",
    "#     if location !=\"King County\": continue \n",
    "        \n",
    "    location_metadata = metadata[(metadata.country == \"USA\") & (metadata.state == state) & (metadata.Location_3 == location)]\n",
    "    location_metadata = location_metadata[[\"Accession ID\", \"Collection date\"]]\n",
    "    location_metadata.columns = [\"seq\", \"date\"]\n",
    "#     location_metadata.date = pd.to_datetime(location_metadata.date, errors=\"coerce\")\n",
    "    location_metadata = location_metadata.dropna()\n",
    "    \n",
    "    date_window = 7\n",
    "    date_step_size = 7\n",
    "\n",
    "    for date in pd.date_range(location_metadata.date.min(), location_metadata.date.max(), freq=f\"{date_step_size}D\"):\n",
    "        end_date = date + np.timedelta64(date_window - 1, 'D')\n",
    "        sub_seqs = location_metadata[(location_metadata.date >= date) & (location_metadata.date <= end_date)].seq\n",
    "        sub_seqs = [s for s in sub_seqs if s in msadb]\n",
    "    #     output_directory, seq_file, out_file = prep_gengraph_input_output(date, end_date, sub_seqs)\n",
    "        experiment_name = f\"window_{date_window}_step_{date_step_size}\"\n",
    "        stdate = date.strftime(\"%Y-%m-%d\")\n",
    "        stend_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "#         if f\"{stdate}_{stend_date}\" != \"2020-12-25_2020-12-31\": continue\n",
    "        \n",
    "#         command = run_pipeline(date, end_date, sub_seqs, experiment_name, location)\n",
    "        command = run_jaccard_pipeline(date, end_date, sub_seqs, experiment_name, location)\n",
    "        \n",
    "        if len(sub_seqs) > 0:\n",
    "            if location.replace(\" \", \"_\") + \" \" + f\"{stdate} {stend_date}\" in timeout:\n",
    "#                 print(location, stdate, len(sub_seqs))\n",
    "#                 break\n",
    "                subprocess.run(command)\n",
    "#             pass\n",
    "#         else:\n",
    "#         loc_date_samples.append((location, stdate, len(sub_seqs)))\n",
    "        \n",
    "#         print(command)\n",
    "#         break\n",
    "#     break\n",
    "#         if len(sub_seqs) > 0:\n",
    "#             subprocess.run(command)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(loc_date_samples, open(\"loc_date_samples.pkl\", \"wb\")) #.tofile(\"loc_date_samples.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
